************
Datasets
************

=====================
General
=====================

* **1 Billion Word Language Model Benchmark**: The purpose of the project is to make available a standard training and test setup for language modeling experiments:
  [`Link <http://www.statmt.org/lm-benchmark/>`_]

* **Common Crawl**: The Common Crawl corpus contains petabytes of data collected over the last 7 years. It contains raw web page data, extracted metadata and text extractions:
  [`Link <http://commoncrawl.org/the-data/get-started/>`_]

* **Yelp Open Dataset**: A subset of Yelp's businesses, reviews, and user data for use in personal, educational, and academic purposes:
  [`Link <https://www.yelp.com/dataset>`_]


=====================
Text classification
=====================

* **20 newsgroups** The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups:
  [`Link <http://qwone.com/~jason/20Newsgroups/>`_]

* **Broadcast News** The 1996 Broadcast News Speech Corpus contains a total of 104 hours of broadcasts from ABC, CNN and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts:
  [`Link <https://catalog.ldc.upenn.edu/LDC97S44>`_]

* **The wikitext long term dependency language modeling dataset**: A collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. :
  [`Link <https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset>`_]

=======================
Question Answering
=======================

* **Question Answering Corpus** by Deep Mind and Oxford which is two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites.
  [`Link <https://github.com/deepmind/rc-data>`_]

* **Stanford Question Answering Dataset (SQuAD)** consisting of questions posed by crowdworkers on a set of Wikipedia articles:
  [`Link <https://rajpurkar.github.io/SQuAD-explorer/>`_]

* **Amazon question/answer data** contains Question and Answer data from Amazon, totaling around 1.4 million answered questions:
  [`Link <http://jmcauley.ucsd.edu/data/amazon/qa/>`_]



=====================
Sentiment Analysis
=====================

* **Multi-Domain Sentiment Dataset** TThe Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains):
  [`Link <http://www.cs.jhu.edu/~mdredze/datasets/sentiment/>`_]

* **Stanford Sentiment Treebank Dataset** The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language:
  [`Link <https://nlp.stanford.edu/sentiment/>`_]

* **Large Movie Review Dataset**: This is a dataset for binary sentiment classification:
  [`Link <http://ai.stanford.edu/~amaas/data/sentiment/>`_]


=====================
Machine Translation
=====================

* **Aligned Hansards of the 36th Parliament of Canada** dataset contains 1.3 million pairs of aligned text chunks:
  [`Link <https://www.isi.edu/natural-language/download/hansard/>`_]

* **Europarl: A Parallel Corpus for Statistical Machine Translation** dataset extracted from the proceedings of the European Parliament:
  [`Link <http://www.statmt.org/europarl/>`_]


=====================
Summarization
=====================

* **Legal Case Reports Data Set** as a textual corpus of 4000 legal cases for automatic summarization and citation analysis.:
  [`Link <https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports>`_]

