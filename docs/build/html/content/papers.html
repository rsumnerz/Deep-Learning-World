

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Papers &mdash; Deep-Learning-World 1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Courses" href="courses.html" />
    <link rel="prev" title="Introduction" href="../intro/intro.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Foreword</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/intro.html">Introduction</a></li>
</ul>
<p class="caption"><span class="caption-text">Content</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Papers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convolutional-networks">Convolutional Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recurrent-networks">Recurrent Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#autoencoders">Autoencoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generative-models">Generative Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probabilistic-models">Probabilistic Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#core">Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#optimization">Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#representation-learning">Representation Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#understanding-and-transfer-learning">Understanding and Transfer Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reinforcement-learning">Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#image-recognition">Image Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#object-recognition">Object Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#action-recognition">Action Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#caption-generation">Caption Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#natural-language-processing">Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#speech-technology">Speech Technology</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="courses.html">Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Document Credentials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../credentials/CONTRIBUTING.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credentials/CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credentials/LICENSE.html">LICENSE</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Deep-Learning-World</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Papers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com///blob/content/papers.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="papers">
<h1>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h1>
<img alt="../_images/article.jpeg" src="../_images/article.jpeg" />
<<p>This chapter is associated with the papers published in deep learning.</p>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolutional-networks">
<h3>Convolutional Networks<a class="headerlink" href="#convolutional-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/convolutional.png" src="../_images/convolutional.png" />
</div></blockquote>
<ul>
<li><p class="first"><strong>Imagenet classification with deep convolutional neural networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Large-scale Video Classification with Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Deep convolutional neural networks for LVCSR</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6639347/&amp;hl=zh-CN&amp;sa=T&amp;oi=gsb&amp;ct=res&amp;cd=0&amp;ei=KknXWYbGFMbFjwSsyICADQ&amp;scisig=AAGBfm2F0Zlu0ciUwadzshNNm80IQQhuhA">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Face recognition: a convolutional neural-network approach</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/554195/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="recurrent-networks">
<h3>Recurrent Networks<a class="headerlink" href="#recurrent-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/Recurrent_neural_network_unfold.svg" src="../_images/Recurrent_neural_network_unfold.svg" /></div></blockquote>
<ul>
<li><p class="first"><strong>An empirical exploration of recurrent network architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&amp;utm_medium=Newsletter&amp;utm_source=revue">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>LSTM: A search space odyssey</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/7508408/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>On the difficulty of training recurrent neural networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning to forget: Continual prediction with LSTM</strong> :
[<a class="reference external" href="http://digital-library.theiet.org/content/conferences/10.1049/cp_19991218">Paper</a>]</p>
>>>>>>>+HEAD
===
<p>T<p>This chapter is associated with the papers published in NLP using deep learning.</p>
<div class="section" id="data-representation">
<h2>Data Representation<a class="headerlink" href="#data-representation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="one-hot-representation">
<h3>One-hot representation<a class="headerlink" href="#one-hot-representation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Character-level convolutional networks for text classification</strong> :
Promising results by the use of one-hot encoding possibly due to their character-level information.
[<a class="reference external" href="http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica">Paper link</a> ,
<a class="reference external" href="https://github.com/zhangxiangxiao/Crepe">Torch implementation</a> ,
<a class="reference external" href="https://github.com/mhjabreel/CharCNN">TensorFlow implementation</a> ,
<a class="reference external" href="https://github.com/srviest/char-cnn-pytorch">Pytorch implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Effective Use of Word Order for Text Categorization with Convolutional Neural Networks</strong> :
Exploiting the 1D structure (namely, word order) of text data for prediction.
[<a class="reference external" href="https://arxiv.org/abs/1412.1058">Paper link</a> ,
<a class="reference external" href="https://github.com/riejohnson/ConText">Code implementation</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Neural Responding Machine for Short-Text Conversation</strong> :
Neural Responding Machine has been proposed to generate content-wise appropriate responses to input text.
[<a class="reference external" href="https://arxiv.org/abs/1503.02364">Paper link</a> ,
<a class="reference external" href="https://isaacchanghau.github.io/2017/07/19/Neural-Responding-Machine-for-Short-Text-Conversation/">Paper summary</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="continuous-bag-of-words-cbow">
<h3>Continuous Bag of Words (CBOW)<a class="headerlink" href="#continuous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Distributed Representations of Words and Phrases and their Compositionality</strong> :
Not necessarily about CBOWs but the techniques represented in this paper
can be used for training the continuous bag-of-words model.
[<a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases">Paper link</a> ,
<a class="reference external" href="https://code.google.com/archive/p/word2vec/">Code implementation 1</a>,
<a class="reference external" href="https://github.com/deborausujono/word2vecpy">Code implementation 2</a>]</p>
>>>>>>>-57bfa4a
t="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Autoencoder_structure.png" src="../_images/Autoencoder_structure.png" />
<ul>
<li><p class="first"><strong>Extracting and composing robust features with denoising autoencoders</strong> :
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=1390294">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/v11/vincent10a.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Adversarial Autoencoders</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.05644">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Autoencoders, Unsupervised Learning, and Deep Architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Reducing the Dimensionality of Data with Neural Networks</strong> :
[<a class="reference external" href="http://science.sciencemag.org/content/313/5786/504">Paper</a>]</p>
>>>>>>>+HEAD
<div cla<div class="section" id="word-level-embedding">
<h3>Word-Level Embedding<a class="headerlink" href="#word-level-embedding" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Efficient Estimation of Word Representations in Vector Space</strong> :
Two novel model architectures for computing continuous vector representations of words.
[<a class="reference external" href="https://arxiv.org/abs/1301.3781">Paper link</a> ,
<a class="reference external" href="https://code.google.com/archive/p/word2vec/">Official code implementation</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>GloVe: Global Vectors for Word Representation</strong> :
Combines the advantages of the two major models of global matrix
factorization and local context window methods and efficiently leverages
the statistical information of the content.
[<a class="reference external" href="http://www.aclweb.org/anthology/D14-1162">Paper link</a> ,
<a class="reference external" href="https://github.com/stanfordnlp/GloVe">Official code implementation</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Skip-Thought Vectors</strong> :
Skip-thought model applies word2vec at the sentence-level.
[<a class="reference external" href="http://papers.nips.cc/paper/5950-skip-thought-vectors">Paper</a> ,
<a class="reference external" href="https://github.com/ryankiros/skip-thoughts">Code implementation</a>,
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/skip_thoughts">TensorFlow implementation</a>]</p>
>>>>>>>-57bfa4a
"../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<<<<<<< H<div class="section" id="generative-models">
<h3>Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/generative.png" src="../_images/generative.png" />
<ul>
<li><p class="first"><strong>Exploiting generative models discriminative classifiers</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Semi-supervised Learning with Deep Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Generalized Denoising Auto-Encoders as Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="probabilistic-models">
<h3>Probabilistic Models<a class="headerlink" href="#probabilistic-models" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1401.4082">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Probabilistic models of cognition: exploring representations and inductive biases</strong> :
[<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1364661310001129">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>On deep generative models with applications to recognition</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5995710/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="core">
<h2>Core<a class="headerlink" href="#core" title="Permalink to this headline">¶</a></h2>
<div class="section" id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.03167">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Training Very Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Large Scale Distributed Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="representation-learning">
<h3>Representation Learning<a class="headerlink" href="#representation-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06434">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Representation Learning: A Review and New Perspectives</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6472238/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation">Paper</a>]</p>
>>>>>>>+HEAD
lass="se<div class="section" id="character-level-embedding">
<h3>Character-Level Embedding<a class="headerlink" href="#character-level-embedding" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning Character-level Representations for Part-of-Speech Tagging</strong> :
CNNs have successfully been utilized for learning character-level embedding.
[<a class="reference external" href="http://proceedings.mlr.press/v32/santos14.pdf">Paper link</a> ]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Deep Convolutional Neural Networks forSentiment Analysis of Short Texts</strong> :
A new deep convolutional neural network has been proposed for exploiting
the character- to sentence-level information for sentiment analysis application on short texts.
[<a class="reference external" href="http://www.aclweb.org/anthology/C14-1008">Paper link</a> ]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</strong> :
The usage of two LSTMs operate over the char-
acters for generating the word embedding
[<a class="reference external" href="https://arxiv.org/abs/1508.02096">Paper link</a> ]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs</strong> :
The effectiveness of modeling characters for dependency parsing.
[<a class="reference external" href="https://arxiv.org/abs/1508.00657">Paper link</a> ]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="part-of-speech-tagging">
<h3>Part-Of-Speech Tagging<a class="headerlink" href="#part-of-speech-tagging" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning Character-level Representations for Part-of-Speech Tagging</strong> :
A deep neural network (DNN) architecture that joins word-level and character-level representations to perform POS taggin
[<a class="reference external" href="http://proceedings.mlr.press/v32/santos14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Bidirectional LSTM-CRF Models for Sequence Tagging</strong> :
A variety of neural network based models haves been proposed for sequence tagging task.
[<a class="reference external" href="https://arxiv.org/abs/1508.01991">Paper</a>,
<a class="reference external" href="https://github.com/Hironsan/anago">Code Implementation 1</a>,
<a class="reference external" href="https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf">Code Implementation 2</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Globally Normalized Transition-Based Neural Networks</strong> :
Transition-based neural network model for part-of-speech tagging.
[<a class="reference external" href="https://arxiv.org/abs/1603.06042">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="parsing">
<h3>Parsing<a class="headerlink" href="#parsing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>A fast and accurate dependency parser using neural networks</strong> :
A novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser.
[<a class="reference external" href="http://www.aclweb.org/anthology/D14-1082">Paper</a>,
<a class="reference external" href="https://github.com/akjindal53244/dependency_parsing_tf">Code Implementation 1</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</strong> :
A simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs.
[<a class="reference external" href="https://arxiv.org/abs/1603.04351">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Transition-Based Dependency Parsing with Stack Long Short-Term Memory</strong> :
A technique for learning representations of parser states in transition-based dependency parsers.
[<a class="reference external" href="https://arxiv.org/abs/1505.08075">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Biaffine Attention for Neural Dependency Parsing</strong> :
Using neural attention in a simple graph-based dependency parser.
[<a class="reference external" href="https://arxiv.org/abs/1611.01734">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
<li><p class="first"><strong>Joint RNN-Based Greedy Parsing and Word Composition</strong> :
A greedy parser based on neural networks, which leverages a new compositional sub-tree representation.
[<a class="reference external" href="https://arxiv.org/abs/1412.7028">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="named-entity-recognition">
<h3>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Neural Architectures for Named Entity Recognition</strong> :
Bidirectional LSTMs and conditional random fields for NER.
[<a class="reference external" href="https://arxiv.org/abs/1603.01360">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Boosting named entity recognition with neural character embeddings</strong> :
A language-independent NER system that uses automatically learned features.
[<a class="reference external" href="https://arxiv.org/abs/1505.05008">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Named Entity Recognition with Bidirectional LSTM-CNNs</strong> :
A novel neural network architecture that automatically detects word- and character-level features.
[<a class="reference external" href="https://arxiv.org/abs/1511.08308">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="semantic-role-labeling">
<h3>Semantic Role Labeling<a class="headerlink" href="#semantic-role-labeling" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>End-to-end learning of semantic role labeling using recurrent neural networks</strong> :
The use of deep bi-directional recurrent network as an end-to-end system for SRL.
[<a class="reference external" href="http://www.aclweb.org/anthology/P15-1109">Paper</a>]</p>
>>>>>>>-57bfa4a
/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<<<<<<< HEAD
<div cl<div class="section" id="understanding-and-transfer-learning">
<h3>Understanding and Transfer Learning<a class="headerlink" href="#understanding-and-transfer-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Distilling the Knowledge in a Neural Network</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1503.02531">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/donahue14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>How transferable are features in deep neural networks?</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-n%E2%80%A6">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="reinforcement-learning">
<h3>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Human-level control through deep reinforcement learning</strong> :
[<a class="reference external" href="https://www.nature.com/articles/nature14236/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Playing Atari with Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.5602">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Continuous control with deep reinforcement learning</strong> :
[<a href="#id76"><span class="problematic" id="id77">`Paper &lt;https://arxiv.org/abs/1509.02971`_</span></a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Reinforcement Learning with Double Q-Learning</strong> :
[<a class="reference external" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Dueling Network Architectures for Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06581">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="image-recognition">
<h3>Image Recognition<a class="headerlink" href="#image-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Residual Learning for Image Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Very Deep Convolutional Networks for Large-Scale Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1409.1556">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Multi-column Deep Neural Networks for Image Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1202.2745">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeepID3: Face Recognition with Very Deep Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.00873">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6034">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Image: Scaling up Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="object-recognition">
<h3>Object Recognition<a class="headerlink" href="#object-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Deep Features for Scene Recognition using Places Database</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5349-learning-deep-features">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Scalable Object Detection using Deep Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6229">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>What is the best multi-stage architecture for object recognition?</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5459469/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="action-recognition">
<h3>Action Recognition<a class="headerlink" href="#action-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Spatiotemporal Features With 3D Convolutional Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Describing Videos by Exploiting Temporal Structure</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Yao_Describing_Videos_by_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Convolutional Two-Stream Network Fusion for Video Action Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Temporal segment networks: Towards good practices for deep action recognition</strong> :
[<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="caption-generation">
<h3>Caption Generation<a class="headerlink" href="#caption-generation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/xuc15.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Chen_Minds_Eye_A_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Text to Image Synthesis</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/reed16.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Visual-Semantic Al60ignments for Generating Image Descriptions</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Show and Tell: A Neural Image Caption Generator</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html">Paper</a>]</p>
>>>>>>>+HEAD
id="text<div class="section" id="text-classification">
<h3>Text classification<a class="headerlink" href="#text-classification" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
By training the model on top of the pretrained word-vectors through finetuning, considerable improvement has been reported for learning task-specific vectors.
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper link</a> ,
<a class="reference external" href="https://github.com/yoonkim/CNN_sentence">Code implementation 1</a>,
<a class="reference external" href="https://github.com/abhaikollara/CNN-Sentence-Classification">Code implementation 2</a>,
<a class="reference external" href="https://github.com/Shawn1993/cnn-text-classification-pytorch">Code implementation 3</a>,
<a class="reference external" href="https://github.com/mangate/ConvNetSent">Code implementation 4</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A Convolutional Neural Network for Modelling Sentences</strong> :
Dynamic Convolutional Neural Network (DCNN) architecture, which technically is the CNN with a dynamic
k-max pooling method, has been proposed for capturing the semantic modeling of the sentences.
[<a class="reference external" href="https://arxiv.org/abs/1404.2188">Paper link</a> ,
<a class="reference external" href="https://github.com/FredericGodin/DynamicCNN">Code implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Very Deep Convolutional Networks for Text Classification</strong> :
The Very Deep Convolutional Neural
Networks (VDCNNs) has been presented and employed at
character-level with the demonstration of the effectiveness of
the network depth on classification tasks
[<a class="reference external" href="http://www.aclweb.org/anthology/E17-1104">Paper link</a> ]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Character-level convolutional networks for text classification</strong> :
The character-level
representation using CNNs investigated which argues
the power of CNNs as well as character-level representation for
language-agnostic text classification.
[<a class="reference external" href="http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica">Paper link</a> ,
<a class="reference external" href="https://github.com/zhangxiangxiao/Crepe">Torch implementation</a> ,
<a class="reference external" href="https://github.com/mhjabreel/CharCNN">TensorFlow implementation</a> ,
<a class="reference external" href="https://github.com/srviest/char-cnn-pytorch">Pytorch implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Multichannel Variable-Size Convolution for Sentence Classification</strong> :
Multichannel Variable Size Convolutional Neural Network (MV-CNN) architecture
Combines different version of word-embeddings in addition to
employing variable-size convolutional filters and is proposed
in this paper for sentence classification.
[<a class="reference external" href="https://arxiv.org/abs/1603.04513">Paper link</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</strong> :
A practical sensitivity analysis of CNNs for exploring the effect
of architecture on the performance, has been investigated in this paper.
[<a class="reference external" href="https://arxiv.org/abs/1510.03820">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Generative and Discriminative Text Classification with Recurrent Neural Networks</strong> :
RNN-based discriminative and generative models have been investigated for
text classification and their robustness to the data distribution shifts has been
claimed as well.
[<a class="reference external" href="https://arxiv.org/abs/1703.01898">Paper link</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</strong> :
An LSTM-RNN architecture has been utilized
for sentence embedding with special superiority in
a defined web search task.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=2992457">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Hierarchical attention networks for document classification</strong> :
Hierarchical
Attention Network (HAN) has been presented and utilized to
capture the hierarchical structure of the text by two word-
level and sentence-level attention mechanism.
[<a class="reference external" href="http://www.aclweb.org/anthology/N16-1174">Paper link</a> ,
<a class="reference external" href="https://github.com/richliao/textClassifier">Code implementation 1</a> ,
<a class="reference external" href="https://github.com/ematvey/hierarchical-attention-networks">Code implementation 2</a> ,
<a class="reference external" href="https://github.com/EdGENetworks/attention-networks-for-classification">Code implementation 3</a>,
<a class="reference external" href="https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/">Summary 1</a>,
<a class="reference external" href="https://medium.com/&#64;sharaf/a-paper-a-day-25-hierarchical-attention-networks-for-document-classification-dd76ba88f176">Summary 2</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Recurrent Convolutional Neural Networks for Text Classification</strong> :
The combination of both RNNs and CNNs is used for text classification which technically
is a recurrent architecture in addition to max-pooling with
an effective word representation method and demonstrates
superiority compared to simple windows-based neural network
approaches.
[<a class="reference external" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">Paper link</a> ,
<a class="reference external" href="https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier">Code implementation 1</a> ,
<a class="reference external" href="https://github.com/knok/rcnn-text-classification">Code implementation 2</a> ,
<a class="reference external" href="https://medium.com/paper-club/recurrent-convolutional-neural-networks-for-text-classification-107020765e52">Summary</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A C-LSTM Neural Network for Text Classification</strong> :
A unified architecture proposed for sentence and document modeling for classification.
[<a class="reference external" href="https://arxiv.org/abs/1511.08630">Paper link</a> ]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="sentiment-analysis">
<h3>Sentiment Analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Domain adaptation for large-scale sentiment classification: A deep learning approach</strong> :
A deep learning approach which learns to extract a meaningful representation for each online review.
[<a class="reference external" href="http://www.iro.umontreal.ca/~lisa/bib/pub_subject/language/pointeurs/ICML2011_sentiment.pdf">Paper link</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Sentiment analysis: Capturing favorability using natural language processing</strong> :
A sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=945658">Paper link</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Document-level sentiment classification: An empirical comparison between SVM and ANN</strong> :
A comparison study. [<a class="reference external" href="https://dl.acm.org/citation.cfm?id=945658">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Learning semantic representations of users and products for document level sentiment classification</strong> :
Incorporating of user- and product- level information into a neural network approach for document level sentiment classification.
[<a class="reference external" href="http://www.aclweb.org/anthology/P15-1098">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Document modeling with gated recurrent neural network for sentiment classification</strong> :
A a neural network model has been proposed to learn vector-based document representation.
[<a class="reference external" href="http://www.aclweb.org/anthology/D15-1167">Paper</a>,
<a class="reference external" href="https://github.com/NUSTM/DLSC">Implementation</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Semi-supervised recursive autoencoders for predicting sentiment distributions</strong> :
A novel machine learning framework based on recursive autoencoders for sentence-level prediction.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=2145450">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>A convolutional neural network for modelling sentences</strong> :
A convolutional architecture adopted for the semantic modelling of sentences.
[<a class="reference external" href="https://arxiv.org/abs/1404.2188">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Recursive deep models for semantic compositionality over a sentiment treebank</strong> :
Recursive Neural Tensor Network for sentiment analysis.
[<a class="reference external" href="http://www.aclweb.org/anthology/D13-1170">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Adaptive recursive neural network for target-dependent twitter sentiment classification</strong> :
AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships.
[<a class="reference external" href="http://www.aclweb.org/anthology/P14-2009">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
<li><p class="first"><strong>Aspect extraction for opinion mining with a deep convolutional neural network</strong> :
A deep learning approach to aspect extraction in opinion mining.
[<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0950705116301721">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="machine-translation">
<h3>Machine Translation<a class="headerlink" href="#machine-translation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong> :
The proposed RNN Encoder–Decoder with a novel hidden unit has been empirically evaluated on the task of machine translation.
[<a class="reference external" href="https://arxiv.org/abs/1406.1078">Paper</a>,
<a class="reference external" href="https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py">Code</a>,
<a class="reference external" href="https://medium.com/&#64;gautam.karmakar/learning-phrase-representation-using-rnn-encoder-decoder-for-machine-translation-9171cd6a6574">Blog post</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Sequence to Sequence Learning with Neural Networks</strong> :
A showcase of NMT system is comparable to the traditional pipeline by Google.
[<a class="reference external" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural">Paper</a>,
<a class="reference external" href="https://github.com/farizrahman4u/seq2seq">Code</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong> :
This work presents the design and implementation of GNMT, a production NMT system at Google.
[<a class="reference external" href="https://arxiv.org/pdf/1609.08144.pdf">Paper</a>,
<a class="reference external" href="https://github.com/tensorflow/nmt">Code</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong> :
An extension to the encoder–decoder model which learns to align and translate jointly by attention mechanism.
[<a class="reference external" href="https://arxiv.org/abs/1409.0473">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Effective Approaches to Attention-based Neural Machine Translation</strong> :
Improvement of attention mechanism for NMT.
[<a class="reference external" href="https://arxiv.org/abs/1508.04025">Paper</a>,
<a class="reference external" href="https://github.com/mohamedkeid/Neural-Machine-Translation">Code</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</strong> :
Analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network.
[<a class="reference external" href="https://arxiv.org/abs/1409.12595">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>On Using Very Large Target Vocabulary for Neural Machine Translation</strong> :
A method that allows to use a very large target vocabulary without increasing training complexity.
[<a class="reference external" href="https://arxiv.org/abs/1412.2007">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Convolutional sequence to sequence learning</strong> :
An architecture based entirely on convolutional neural networks.
[<a class="reference external" href="https://arxiv.org/abs/1705.03122">Paper</a>,
<a class="reference external" href="https://github.com/facebookresearch/fairseq">Code[Torch]</a>,
<a class="reference external" href="https://github.com/pytorch/fairseq">Code[Pytorch]</a>,
<a class="reference external" href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/">Post</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Attention Is All You Need</strong> :
The Transformer: a novel neural network architecture based on a self-attention mechanism.
[<a class="reference external" href="https://arxiv.org/abs/1706.03762">Paper</a>,
<a class="reference external" href="https://github.com/tensorflow/tensor2tensor">Code</a>,
<a class="reference external" href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Accelerating Deep Learning Research with the Tensor2Tensor Library</a>,
<a class="reference external" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>]</p>
>>>>>>>-57bfa4a
png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<<<<<<< HEAD
<div class="section" id="natu<div class="section" id="natural-language-processing">
<h3>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Distributed Representations of Words and Phrases and their Compositionality</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Efficient Estimation of Word Representations in Vector Space</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Sequence to Sequence Learning with Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.3215.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.0473.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1704.04368">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Attention Is All You Need</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1706.03762">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="speech-technology">
<h3>Speech Technology<a class="headerlink" href="#speech-technology" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6296526/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Towards End-to-End Speech Recognition with Recurrent Neural Networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/graves14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Speech recognition with deep recurrent neural networks</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6638947/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1507.06947">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>A novel scheme for speaker recognition using a phonetically-aware deep neural network</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6853887/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
>>>>>>>+HEAD
on">
<h3<div class="section" id="summarization">
<h3>Summarization<a class="headerlink" href="#summarization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>A Neural Attention Model for Abstractive Sentence Summarization</strong> :
A fully data-driven approach to abstractive sentence summarization based on a local attention model.
[<a class="reference external" href="https://arxiv.org/abs/1509.00685">Paper</a>,
<a class="reference external" href="https://github.com/facebookarchive/NAMAS">Code</a>,
<a class="reference external" href="http://thegrandjanitor.com/2018/05/09/a-read-on-a-neural-attention-model-for-abstractive-sentence-summarization-by-a-m-rush-sumit-chopra-and-jason-weston/">A Read on “A Neural Attention Model for Abstractive Sentence Summarization”</a>,
<a class="reference external" href="https://medium.com/&#64;supersonic_ss/paper-a-neural-attention-model-for-abstractive-sentence-summarization-a6fa9b33f09b">Blog Post</a>,
<a class="reference external" href="https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/neural-attention-model-for-abstractive-sentence-summarization.md">Paper notes</a>,]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong> :
A novel architecture that augments the standard sequence-to-sequence attentional model by using a hybrid pointer-generator network that may copy words from the source text via pointing and using coverage to keep track of what has been summarized.
[<a class="reference external" href="https://arxiv.org/abs/1704.04368">Paper</a>,
<a class="reference external" href="https://github.com/abisee/pointer-generator">Code</a>,
<a class="reference external" href="https://www.coursera.org/lecture/language-processing/get-to-the-point-summarization-with-pointer-generator-networks-RhxPO">Video</a>,
<a class="reference external" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html">Blog Post</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</strong> :
A  conditional  recurrent  neural  network (RNN) based on convolutional attention-based encoder which generates a summary of an input sentence.
[<a class="reference external" href="http://www.aclweb.org/anthology/N16-1012">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</strong> :
Abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks
[<a class="reference external" href="https://arxiv.org/abs/1602.06023">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>A Deep Reinforced Model for Abstractive Summarization</strong> :
A neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL).
[<a class="reference external" href="https://arxiv.org/abs/1705.04304">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="question-answering">
<h3>Question Answering<a class="headerlink" href="#question-answering" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</strong> :
An argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering.
[<a class="reference external" href="https://arxiv.org/abs/1502.05698">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Teaching Machines to Read and Comprehend</strong> :
addressing the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set.
[<a class="reference external" href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Ask Me Anything Dynamic Memory Networks for Natural Language Processing</strong> :
Introducing the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers
[<a class="reference external" href="http://proceedings.mlr.press/v48/kumar16.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
>>>>>>>-57bfa4a
        
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="courses.html" class="btn btn-neutral float-right" title="Courses" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../intro/intro.html" class="btn btn-neutral" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Amirsina Torfi.
      Last updated on True.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Papers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com///blob/content/papers.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="papers">
<h1>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h1>
<img alt="../_images/article.jpeg" src="../_images/article.jpeg" />
<<p>This chapter is associated with the papers published in deep learning.</p>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolutional-networks">
<h3>Convolutional Networks<a class="headerlink" href="#convolutional-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/convolutional.png" src="../_images/convolutional.png" />
</div></blockquote>
<ul>
<li><p class="first"><strong>Imagenet classification with deep convolutional neural networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Large-scale Video Classification with Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Deep convolutional neural networks for LVCSR</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6639347/&amp;hl=zh-CN&amp;sa=T&amp;oi=gsb&amp;ct=res&amp;cd=0&amp;ei=KknXWYbGFMbFjwSsyICADQ&amp;scisig=AAGBfm2F0Zlu0ciUwadzshNNm80IQQhuhA">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Face recognition: a convolutional neural-network approach</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/554195/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="recurrent-networks">
<h3>Recurrent Networks<a class="headerlink" href="#recurrent-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/Recurrent_neural_network_unfold.svg" src="../_images/Recurrent_neural_network_unfold.svg" /></div></blockquote>
<ul>
<li><p class="first"><strong>An empirical exploration of recurrent network architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&amp;utm_medium=Newsletter&amp;utm_source=revue">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>LSTM: A search space odyssey</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/7508408/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>On the difficulty of training recurrent neural networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning to forget: Continual prediction with LSTM</strong> :
[<a class="reference external" href="http://digital-library.theiet.org/content/conferences/10.1049/cp_19991218">Paper</a>]</p>
>>>>>>>+HEAD
===
<p>T<p>This chapter is associated with the papers published in NLP using deep learning.</p>
<div class="section" id="data-representation">
<h2>Data Representation<a class="headerlink" href="#data-representation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="one-hot-representation">
<h3>One-hot representation<a class="headerlink" href="#one-hot-representation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Character-level convolutional networks for text classification</strong> :
Promising results by the use of one-hot encoding possibly due to their character-level information.
[<a class="reference external" href="http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica">Paper link</a> ,
<a class="reference external" href="https://github.com/zhangxiangxiao/Crepe">Torch implementation</a> ,
<a class="reference external" href="https://github.com/mhjabreel/CharCNN">TensorFlow implementation</a> ,
<a class="reference external" href="https://github.com/srviest/char-cnn-pytorch">Pytorch implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Effective Use of Word Order for Text Categorization with Convolutional Neural Networks</strong> :
Exploiting the 1D structure (namely, word order) of text data for prediction.
[<a class="reference external" href="https://arxiv.org/abs/1412.1058">Paper link</a> ,
<a class="reference external" href="https://github.com/riejohnson/ConText">Code implementation</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Neural Responding Machine for Short-Text Conversation</strong> :
Neural Responding Machine has been proposed to generate content-wise appropriate responses to input text.
[<a class="reference external" href="https://arxiv.org/abs/1503.02364">Paper link</a> ,
<a class="reference external" href="https://isaacchanghau.github.io/2017/07/19/Neural-Responding-Machine-for-Short-Text-Conversation/">Paper summary</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="continuous-bag-of-words-cbow">
<h3>Continuous Bag of Words (CBOW)<a class="headerlink" href="#continuous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Distributed Representations of Words and Phrases and their Compositionality</strong> :
Not necessarily about CBOWs but the techniques represented in this paper
can be used for training the continuous bag-of-words model.
[<a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases">Paper link</a> ,
<a class="reference external" href="https://code.google.com/archive/p/word2vec/">Code implementation 1</a>,
<a class="reference external" href="https://github.com/deborausujono/word2vecpy">Code implementation 2</a>]</p>
>>>>>>>-57bfa4a
t="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<<<<<<<<div class="section" id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Autoencoder_structure.png" src="../_images/Autoencoder_structure.png" />
<ul>
<li><p class="first"><strong>Extracting and composing robust features with denoising autoencoders</strong> :
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=1390294">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/v11/vincent10a.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Adversarial Autoencoders</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.05644">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Autoencoders, Unsupervised Learning, and Deep Architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Reducing the Dimensionality of Data with Neural Networks</strong> :
[<a class="reference external" href="http://science.sciencemag.org/content/313/5786/504">Paper</a>]</p>
>>>>>>>+HEAD
<div cla<div class="section" id="word-level-embedding">
<h3>Word-Level Embedding<a class="headerlink" href="#word-level-embedding" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Efficient Estimation of Word Representations in Vector Space</strong> :
Two novel model architectures for computing continuous vector representations of words.
[<a class="reference external" href="https://arxiv.org/abs/1301.3781">Paper link</a> ,
<a class="reference external" href="https://code.google.com/archive/p/word2vec/">Official code implementation</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>GloVe: Global Vectors for Word Representation</strong> :
Combines the advantages of the two major models of global matrix
factorization and local context window methods and efficiently leverages
the statistical information of the content.
[<a class="reference external" href="http://www.aclweb.org/anthology/D14-1162">Paper link</a> ,
<a class="reference external" href="https://github.com/stanfordnlp/GloVe">Official code implementation</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Skip-Thought Vectors</strong> :
Skip-thought model applies word2vec at the sentence-level.
[<a class="reference external" href="http://papers.nips.cc/paper/5950-skip-thought-vectors">Paper</a> ,
<a class="reference external" href="https://github.com/ryankiros/skip-thoughts">Code implementation</a>,
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/skip_thoughts">TensorFlow implementation</a>]</p>
>>>>>>>-57bfa4a
"../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<<<<<<< H<div class="section" id="generative-models">
<h3>Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/generative.png" src="../_images/generative.png" />
<ul>
<li><p class="first"><strong>Exploiting generative models discriminative classifiers</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Semi-supervised Learning with Deep Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Generalized Denoising Auto-Encoders as Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="probabilistic-models">
<h3>Probabilistic Models<a class="headerlink" href="#probabilistic-models" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1401.4082">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Probabilistic models of cognition: exploring representations and inductive biases</strong> :
[<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1364661310001129">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>On deep generative models with applications to recognition</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5995710/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="core">
<h2>Core<a class="headerlink" href="#core" title="Permalink to this headline">¶</a></h2>
<div class="section" id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.03167">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Training Very Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Large Scale Distributed Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="representation-learning">
<h3>Representation Learning<a class="headerlink" href="#representation-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06434">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Representation Learning: A Review and New Perspectives</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6472238/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation">Paper</a>]</p>
>>>>>>>+HEAD
lass="se<div class="section" id="character-level-embedding">
<h3>Character-Level Embedding<a class="headerlink" href="#character-level-embedding" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning Character-level Representations for Part-of-Speech Tagging</strong> :
CNNs have successfully been utilized for learning character-level embedding.
[<a class="reference external" href="http://proceedings.mlr.press/v32/santos14.pdf">Paper link</a> ]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Deep Convolutional Neural Networks forSentiment Analysis of Short Texts</strong> :
A new deep convolutional neural network has been proposed for exploiting
the character- to sentence-level information for sentiment analysis application on short texts.
[<a class="reference external" href="http://www.aclweb.org/anthology/C14-1008">Paper link</a> ]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</strong> :
The usage of two LSTMs operate over the char-
acters for generating the word embedding
[<a class="reference external" href="https://arxiv.org/abs/1508.02096">Paper link</a> ]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs</strong> :
The effectiveness of modeling characters for dependency parsing.
[<a class="reference external" href="https://arxiv.org/abs/1508.00657">Paper link</a> ]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="part-of-speech-tagging">
<h3>Part-Of-Speech Tagging<a class="headerlink" href="#part-of-speech-tagging" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning Character-level Representations for Part-of-Speech Tagging</strong> :
A deep neural network (DNN) architecture that joins word-level and character-level representations to perform POS taggin
[<a class="reference external" href="http://proceedings.mlr.press/v32/santos14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Bidirectional LSTM-CRF Models for Sequence Tagging</strong> :
A variety of neural network based models haves been proposed for sequence tagging task.
[<a class="reference external" href="https://arxiv.org/abs/1508.01991">Paper</a>,
<a class="reference external" href="https://github.com/Hironsan/anago">Code Implementation 1</a>,
<a class="reference external" href="https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf">Code Implementation 2</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Globally Normalized Transition-Based Neural Networks</strong> :
Transition-based neural network model for part-of-speech tagging.
[<a class="reference external" href="https://arxiv.org/abs/1603.06042">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="parsing">
<h3>Parsing<a class="headerlink" href="#parsing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>A fast and accurate dependency parser using neural networks</strong> :
A novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser.
[<a class="reference external" href="http://www.aclweb.org/anthology/D14-1082">Paper</a>,
<a class="reference external" href="https://github.com/akjindal53244/dependency_parsing_tf">Code Implementation 1</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</strong> :
A simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs.
[<a class="reference external" href="https://arxiv.org/abs/1603.04351">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Transition-Based Dependency Parsing with Stack Long Short-Term Memory</strong> :
A technique for learning representations of parser states in transition-based dependency parsers.
[<a class="reference external" href="https://arxiv.org/abs/1505.08075">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Biaffine Attention for Neural Dependency Parsing</strong> :
Using neural attention in a simple graph-based dependency parser.
[<a class="reference external" href="https://arxiv.org/abs/1611.01734">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
<li><p class="first"><strong>Joint RNN-Based Greedy Parsing and Word Composition</strong> :
A greedy parser based on neural networks, which leverages a new compositional sub-tree representation.
[<a class="reference external" href="https://arxiv.org/abs/1412.7028">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="named-entity-recognition">
<h3>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Neural Architectures for Named Entity Recognition</strong> :
Bidirectional LSTMs and conditional random fields for NER.
[<a class="reference external" href="https://arxiv.org/abs/1603.01360">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Boosting named entity recognition with neural character embeddings</strong> :
A language-independent NER system that uses automatically learned features.
[<a class="reference external" href="https://arxiv.org/abs/1505.05008">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Named Entity Recognition with Bidirectional LSTM-CNNs</strong> :
A novel neural network architecture that automatically detects word- and character-level features.
[<a class="reference external" href="https://arxiv.org/abs/1511.08308">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="semantic-role-labeling">
<h3>Semantic Role Labeling<a class="headerlink" href="#semantic-role-labeling" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>End-to-end learning of semantic role labeling using recurrent neural networks</strong> :
The use of deep bi-directional recurrent network as an end-to-end system for SRL.
[<a class="reference external" href="http://www.aclweb.org/anthology/P15-1109">Paper</a>]</p>
>>>>>>>-57bfa4a
/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<<<<<<< HEAD
<div cl<div class="section" id="understanding-and-transfer-learning">
<h3>Understanding and Transfer Learning<a class="headerlink" href="#understanding-and-transfer-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Distilling the Knowledge in a Neural Network</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1503.02531">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/donahue14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>How transferable are features in deep neural networks?</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-n%E2%80%A6">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="reinforcement-learning">
<h3>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Human-level control through deep reinforcement learning</strong> :
[<a class="reference external" href="https://www.nature.com/articles/nature14236/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Playing Atari with Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.5602">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Continuous control with deep reinforcement learning</strong> :
[<a href="#id76"><span class="problematic" id="id77">`Paper &lt;https://arxiv.org/abs/1509.02971`_</span></a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Reinforcement Learning with Double Q-Learning</strong> :
[<a class="reference external" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Dueling Network Architectures for Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06581">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="image-recognition">
<h3>Image Recognition<a class="headerlink" href="#image-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Residual Learning for Image Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Very Deep Convolutional Networks for Large-Scale Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1409.1556">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Multi-column Deep Neural Networks for Image Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1202.2745">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeepID3: Face Recognition with Very Deep Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.00873">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6034">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Image: Scaling up Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="object-recognition">
<h3>Object Recognition<a class="headerlink" href="#object-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Deep Features for Scene Recognition using Places Database</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5349-learning-deep-features">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Scalable Object Detection using Deep Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6229">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>What is the best multi-stage architecture for object recognition?</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5459469/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="action-recognition">
<h3>Action Recognition<a class="headerlink" href="#action-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Spatiotemporal Features With 3D Convolutional Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Describing Videos by Exploiting Temporal Structure</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Yao_Describing_Videos_by_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Convolutional Two-Stream Network Fusion for Video Action Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Temporal segment networks: Towards good practices for deep action recognition</strong> :
[<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="caption-generation">
<h3>Caption Generation<a class="headerlink" href="#caption-generation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/xuc15.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Chen_Minds_Eye_A_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Text to Image Synthesis</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/reed16.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Visual-Semantic Al60ignments for Generating Image Descriptions</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Show and Tell: A Neural Image Caption Generator</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html">Paper</a>]</p>
>>>>>>>+HEAD
id="text<div class="section" id="text-classification">
<h3>Text classification<a class="headerlink" href="#text-classification" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
By training the model on top of the pretrained word-vectors through finetuning, considerable improvement has been reported for learning task-specific vectors.
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper link</a> ,
<a class="reference external" href="https://github.com/yoonkim/CNN_sentence">Code implementation 1</a>,
<a class="reference external" href="https://github.com/abhaikollara/CNN-Sentence-Classification">Code implementation 2</a>,
<a class="reference external" href="https://github.com/Shawn1993/cnn-text-classification-pytorch">Code implementation 3</a>,
<a class="reference external" href="https://github.com/mangate/ConvNetSent">Code implementation 4</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A Convolutional Neural Network for Modelling Sentences</strong> :
Dynamic Convolutional Neural Network (DCNN) architecture, which technically is the CNN with a dynamic
k-max pooling method, has been proposed for capturing the semantic modeling of the sentences.
[<a class="reference external" href="https://arxiv.org/abs/1404.2188">Paper link</a> ,
<a class="reference external" href="https://github.com/FredericGodin/DynamicCNN">Code implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Very Deep Convolutional Networks for Text Classification</strong> :
The Very Deep Convolutional Neural
Networks (VDCNNs) has been presented and employed at
character-level with the demonstration of the effectiveness of
the network depth on classification tasks
[<a class="reference external" href="http://www.aclweb.org/anthology/E17-1104">Paper link</a> ]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Character-level convolutional networks for text classification</strong> :
The character-level
representation using CNNs investigated which argues
the power of CNNs as well as character-level representation for
language-agnostic text classification.
[<a class="reference external" href="http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classifica">Paper link</a> ,
<a class="reference external" href="https://github.com/zhangxiangxiao/Crepe">Torch implementation</a> ,
<a class="reference external" href="https://github.com/mhjabreel/CharCNN">TensorFlow implementation</a> ,
<a class="reference external" href="https://github.com/srviest/char-cnn-pytorch">Pytorch implementation</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Multichannel Variable-Size Convolution for Sentence Classification</strong> :
Multichannel Variable Size Convolutional Neural Network (MV-CNN) architecture
Combines different version of word-embeddings in addition to
employing variable-size convolutional filters and is proposed
in this paper for sentence classification.
[<a class="reference external" href="https://arxiv.org/abs/1603.04513">Paper link</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</strong> :
A practical sensitivity analysis of CNNs for exploring the effect
of architecture on the performance, has been investigated in this paper.
[<a class="reference external" href="https://arxiv.org/abs/1510.03820">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Generative and Discriminative Text Classification with Recurrent Neural Networks</strong> :
RNN-based discriminative and generative models have been investigated for
text classification and their robustness to the data distribution shifts has been
claimed as well.
[<a class="reference external" href="https://arxiv.org/abs/1703.01898">Paper link</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</strong> :
An LSTM-RNN architecture has been utilized
for sentence embedding with special superiority in
a defined web search task.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=2992457">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Hierarchical attention networks for document classification</strong> :
Hierarchical
Attention Network (HAN) has been presented and utilized to
capture the hierarchical structure of the text by two word-
level and sentence-level attention mechanism.
[<a class="reference external" href="http://www.aclweb.org/anthology/N16-1174">Paper link</a> ,
<a class="reference external" href="https://github.com/richliao/textClassifier">Code implementation 1</a> ,
<a class="reference external" href="https://github.com/ematvey/hierarchical-attention-networks">Code implementation 2</a> ,
<a class="reference external" href="https://github.com/EdGENetworks/attention-networks-for-classification">Code implementation 3</a>,
<a class="reference external" href="https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/">Summary 1</a>,
<a class="reference external" href="https://medium.com/&#64;sharaf/a-paper-a-day-25-hierarchical-attention-networks-for-document-classification-dd76ba88f176">Summary 2</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>Recurrent Convolutional Neural Networks for Text Classification</strong> :
The combination of both RNNs and CNNs is used for text classification which technically
is a recurrent architecture in addition to max-pooling with
an effective word representation method and demonstrates
superiority compared to simple windows-based neural network
approaches.
[<a class="reference external" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">Paper link</a> ,
<a class="reference external" href="https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier">Code implementation 1</a> ,
<a class="reference external" href="https://github.com/knok/rcnn-text-classification">Code implementation 2</a> ,
<a class="reference external" href="https://medium.com/paper-club/recurrent-convolutional-neural-networks-for-text-classification-107020765e52">Summary</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
<ul>
<li><p class="first"><strong>A C-LSTM Neural Network for Text Classification</strong> :
A unified architecture proposed for sentence and document modeling for classification.
[<a class="reference external" href="https://arxiv.org/abs/1511.08630">Paper link</a> ]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="sentiment-analysis">
<h3>Sentiment Analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Domain adaptation for large-scale sentiment classification: A deep learning approach</strong> :
A deep learning approach which learns to extract a meaningful representation for each online review.
[<a class="reference external" href="http://www.iro.umontreal.ca/~lisa/bib/pub_subject/language/pointeurs/ICML2011_sentiment.pdf">Paper link</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Sentiment analysis: Capturing favorability using natural language processing</strong> :
A sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=945658">Paper link</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Document-level sentiment classification: An empirical comparison between SVM and ANN</strong> :
A comparison study. [<a class="reference external" href="https://dl.acm.org/citation.cfm?id=945658">Paper link</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Learning semantic representations of users and products for document level sentiment classification</strong> :
Incorporating of user- and product- level information into a neural network approach for document level sentiment classification.
[<a class="reference external" href="http://www.aclweb.org/anthology/P15-1098">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Document modeling with gated recurrent neural network for sentiment classification</strong> :
A a neural network model has been proposed to learn vector-based document representation.
[<a class="reference external" href="http://www.aclweb.org/anthology/D15-1167">Paper</a>,
<a class="reference external" href="https://github.com/NUSTM/DLSC">Implementation</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Semi-supervised recursive autoencoders for predicting sentiment distributions</strong> :
A novel machine learning framework based on recursive autoencoders for sentence-level prediction.
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=2145450">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>A convolutional neural network for modelling sentences</strong> :
A convolutional architecture adopted for the semantic modelling of sentences.
[<a class="reference external" href="https://arxiv.org/abs/1404.2188">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Recursive deep models for semantic compositionality over a sentiment treebank</strong> :
Recursive Neural Tensor Network for sentiment analysis.
[<a class="reference external" href="http://www.aclweb.org/anthology/D13-1170">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Adaptive recursive neural network for target-dependent twitter sentiment classification</strong> :
AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships.
[<a class="reference external" href="http://www.aclweb.org/anthology/P14-2009">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
<li><p class="first"><strong>Aspect extraction for opinion mining with a deep convolutional neural network</strong> :
A deep learning approach to aspect extraction in opinion mining.
[<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0950705116301721">Paper</a>]</p>
<img alt="../_images/progress-overall-20.png" src="../_images/progress-overall-20.png" />
</li>
</ul>
</div>
<div class="section" id="machine-translation">
<h3>Machine Translation<a class="headerlink" href="#machine-translation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong> :
The proposed RNN Encoder–Decoder with a novel hidden unit has been empirically evaluated on the task of machine translation.
[<a class="reference external" href="https://arxiv.org/abs/1406.1078">Paper</a>,
<a class="reference external" href="https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py">Code</a>,
<a class="reference external" href="https://medium.com/&#64;gautam.karmakar/learning-phrase-representation-using-rnn-encoder-decoder-for-machine-translation-9171cd6a6574">Blog post</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Sequence to Sequence Learning with Neural Networks</strong> :
A showcase of NMT system is comparable to the traditional pipeline by Google.
[<a class="reference external" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural">Paper</a>,
<a class="reference external" href="https://github.com/farizrahman4u/seq2seq">Code</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong> :
This work presents the design and implementation of GNMT, a production NMT system at Google.
[<a class="reference external" href="https://arxiv.org/pdf/1609.08144.pdf">Paper</a>,
<a class="reference external" href="https://github.com/tensorflow/nmt">Code</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong> :
An extension to the encoder–decoder model which learns to align and translate jointly by attention mechanism.
[<a class="reference external" href="https://arxiv.org/abs/1409.0473">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Effective Approaches to Attention-based Neural Machine Translation</strong> :
Improvement of attention mechanism for NMT.
[<a class="reference external" href="https://arxiv.org/abs/1508.04025">Paper</a>,
<a class="reference external" href="https://github.com/mohamedkeid/Neural-Machine-Translation">Code</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</strong> :
Analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network.
[<a class="reference external" href="https://arxiv.org/abs/1409.12595">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>On Using Very Large Target Vocabulary for Neural Machine Translation</strong> :
A method that allows to use a very large target vocabulary without increasing training complexity.
[<a class="reference external" href="https://arxiv.org/abs/1412.2007">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Convolutional sequence to sequence learning</strong> :
An architecture based entirely on convolutional neural networks.
[<a class="reference external" href="https://arxiv.org/abs/1705.03122">Paper</a>,
<a class="reference external" href="https://github.com/facebookresearch/fairseq">Code[Torch]</a>,
<a class="reference external" href="https://github.com/pytorch/fairseq">Code[Pytorch]</a>,
<a class="reference external" href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/">Post</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Attention Is All You Need</strong> :
The Transformer: a novel neural network architecture based on a self-attention mechanism.
[<a class="reference external" href="https://arxiv.org/abs/1706.03762">Paper</a>,
<a class="reference external" href="https://github.com/tensorflow/tensor2tensor">Code</a>,
<a class="reference external" href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Accelerating Deep Learning Research with the Tensor2Tensor Library</a>,
<a class="reference external" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>]</p>
>>>>>>>-57bfa4a
png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<<<<<<< HEAD
<div class="section" id="natu<div class="section" id="natural-language-processing">
<h3>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Distributed Representations of Words and Phrases and their Compositionality</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Efficient Estimation of Word Representations in Vector Space</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Sequence to Sequence Learning with Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.3215.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.0473.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1704.04368">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Attention Is All You Need</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1706.03762">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="speech-technology">
<h3>Speech Technology<a class="headerlink" href="#speech-technology" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6296526/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Towards End-to-End Speech Recognition with Recurrent Neural Networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/graves14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Speech recognition with deep recurrent neural networks</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6638947/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1507.06947">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>A novel scheme for speaker recognition using a phonetically-aware deep neural network</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6853887/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
>>>>>>>+HEAD
on">
<h3<div class="section" id="summarization">
<h3>Summarization<a class="headerlink" href="#summarization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>A Neural Attention Model for Abstractive Sentence Summarization</strong> :
A fully data-driven approach to abstractive sentence summarization based on a local attention model.
[<a class="reference external" href="https://arxiv.org/abs/1509.00685">Paper</a>,
<a class="reference external" href="https://github.com/facebookarchive/NAMAS">Code</a>,
<a class="reference external" href="http://thegrandjanitor.com/2018/05/09/a-read-on-a-neural-attention-model-for-abstractive-sentence-summarization-by-a-m-rush-sumit-chopra-and-jason-weston/">A Read on “A Neural Attention Model for Abstractive Sentence Summarization”</a>,
<a class="reference external" href="https://medium.com/&#64;supersonic_ss/paper-a-neural-attention-model-for-abstractive-sentence-summarization-a6fa9b33f09b">Blog Post</a>,
<a class="reference external" href="https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/neural-attention-model-for-abstractive-sentence-summarization.md">Paper notes</a>,]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong> :
A novel architecture that augments the standard sequence-to-sequence attentional model by using a hybrid pointer-generator network that may copy words from the source text via pointing and using coverage to keep track of what has been summarized.
[<a class="reference external" href="https://arxiv.org/abs/1704.04368">Paper</a>,
<a class="reference external" href="https://github.com/abisee/pointer-generator">Code</a>,
<a class="reference external" href="https://www.coursera.org/lecture/language-processing/get-to-the-point-summarization-with-pointer-generator-networks-RhxPO">Video</a>,
<a class="reference external" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html">Blog Post</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</strong> :
A  conditional  recurrent  neural  network (RNN) based on convolutional attention-based encoder which generates a summary of an input sentence.
[<a class="reference external" href="http://www.aclweb.org/anthology/N16-1012">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</strong> :
Abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks
[<a class="reference external" href="https://arxiv.org/abs/1602.06023">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>A Deep Reinforced Model for Abstractive Summarization</strong> :
A neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL).
[<a class="reference external" href="https://arxiv.org/abs/1705.04304">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="question-answering">
<h3>Question Answering<a class="headerlink" href="#question-answering" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</strong> :
An argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering.
[<a class="reference external" href="https://arxiv.org/abs/1502.05698">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Teaching Machines to Read and Comprehend</strong> :
addressing the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set.
[<a class="reference external" href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Ask Me Anything Dynamic Memory Networks for Natural Language Processing</strong> :
Introducing the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers
[<a class="reference external" href="http://proceedings.mlr.press/v48/kumar16.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
>>>>>>>-57bfa4a
        
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="courses.html" class="btn btn-neutral float-right" title="Courses" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../intro/intro.html" class="btn btn-neutral" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Amirsina Torfi.
      Last updated on True.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>